import numpy as np
import time
from numba import cuda, njit

# CPU-реализация суммы элементов вектора
@njit
def sum_on_cpu(vector):
    return np.sum(vector)

# GPU-реализация суммы элементов вектора
@cuda.jit
def sum_on_gpu(vector, result):
    shared_sum = cuda.shared.array(shape=1024, dtype=numba.float64)
    tid = cuda.threadIdx.x
    bid = cuda.blockIdx.x
    bdim = cuda.blockDim.x

    idx = bid * bdim + tid

    if idx < vector.size:
        shared_sum[tid] = vector[idx]
    else:
        shared_sum[tid] = 0

    cuda.syncthreads()

    # Редукция суммы внутри блока
    i = bdim // 2
    while i > 0:
        if tid < i:
            shared_sum[tid] += shared_sum[tid + i]
        cuda.syncthreads()
        i //= 2

    if tid == 0:
        result[bid] = shared_sum[0]

def vector_sum_cpu(vector):
    start_time = time.time()
    result = sum_on_cpu(vector)
    elapsed_time = time.time() - start_time
    return result, elapsed_time

def vector_sum_gpu(vector):
    threads_per_block = 1024
    blocks_per_grid = (vector.size + threads_per_block - 1) // threads_per_block

    vector_device = cuda.to_device(vector)
    result_device = cuda.device_array(blocks_per_grid, dtype=np.float64)

    start_time = time.time()
    sum_on_gpu[blocks_per_grid, threads_per_block](vector_device, result_device)
    cuda.synchronize()

    result = result_device.copy_to_host()
    final_sum = np.sum(result)
    elapsed_time = time.time() - start_time

    return final_sum, elapsed_time

if __name__ == "__main__":
    vector_size = 10**6
    vector = np.random.rand(vector_size).astype(np.float64)

    # Сумма на CPU
    cpu_result, cpu_time = vector_sum_cpu(vector)
    print(f"CPU: Сумма = {cpu_result}, Время = {cpu_time:.6f} сек")

    # Сумма на GPU
    gpu_result, gpu_time = vector_sum_gpu(vector)
    print(f"GPU: Сумма = {gpu_result}, Время = {gpu_time:.6f} сек")

# Увеличим размерность

import numpy as np
import time
from numba import cuda, njit
import pandas as pd
import matplotlib.pyplot as plt

# CPU-реализация суммы элементов вектора
@njit
def sum_on_cpu(vector):
    return np.sum(vector)

def vector_sum_cpu(vector):
    start_time = time.time()
    result = sum_on_cpu(vector)
    elapsed_time = time.time() - start_time
    return result, elapsed_time

# GPU-реализация суммы элементов вектора
@cuda.jit
def sum_on_gpu(vector, result):
    shared_sum = cuda.shared.array(shape=1024, dtype=np.float64)
    tid = cuda.threadIdx.x
    bid = cuda.blockIdx.x
    bdim = cuda.blockDim.x

    idx = bid * bdim + tid

    if idx < vector.size:
        shared_sum[tid] = vector[idx]
    else:
        shared_sum[tid] = 0

    cuda.syncthreads()

    # Редукция суммы внутри блока
    i = bdim // 2
    while i > 0:
        if tid < i:
            shared_sum[tid] += shared_sum[tid + i]
        cuda.syncthreads()
        i //= 2

    if tid == 0:
        result[bid] = shared_sum[0]

def vector_sum_gpu(vector):
    threads_per_block = 1024
    blocks_per_grid = (vector.size + threads_per_block - 1) // threads_per_block

    vector_device = cuda.to_device(vector)
    result_device = cuda.device_array(blocks_per_grid, dtype=np.float64)

    start_time = time.time()
    sum_on_gpu[blocks_per_grid, threads_per_block](vector_device, result_device)
    cuda.synchronize()

    result = result_device.copy_to_host()
    final_sum = np.sum(result)
    elapsed_time = time.time() - start_time

    return final_sum, elapsed_time

# Проведение экспериментов на CPU и GPU
def run_experiments():
    experiment_results_cpu = []
    experiment_results_gpu = []

    vector_size = 10**7  # Увеличенный размер вектора
    for i in range(5):
        vector = np.random.rand(vector_size).astype(np.float64)

        # CPU вычисления
        cpu_result, cpu_time = vector_sum_cpu(vector)
        experiment_results_cpu.append({
            "Experiment": i + 1,
            "CPU Time (s)": cpu_time,
            "Sum": cpu_result
        })

        # GPU вычисления
        gpu_result, gpu_time = vector_sum_gpu(vector)
        experiment_results_gpu.append({
            "Experiment": i + 1,
            "GPU Time (s)": gpu_time,
            "Sum": gpu_result
        })

    # Формирование таблиц результатов
    results_df_cpu = pd.DataFrame(experiment_results_cpu)
    results_df_gpu = pd.DataFrame(experiment_results_gpu)

    # Вывод таблиц в формате Markdown
    print("# Результаты экспериментов на CPU\n")
    print(results_df_cpu.to_markdown(index=False))
    print("\n# Результаты экспериментов на GPU\n")
    print(results_df_gpu.to_markdown(index=False))

    # Построение графиков времени выполнения
    plt.figure(figsize=(10, 6))
    plt.plot(results_df_cpu["Experiment"], results_df_cpu["CPU Time (s)"], label="CPU Time (s)", marker='o', color='blue')
    plt.plot(results_df_gpu["Experiment"], results_df_gpu["GPU Time (s)"], label="GPU Time (s)", marker='o', color='green')
    plt.title("Execution Time: CPU vs GPU")
    plt.xlabel("Experiment")
    plt.ylabel("Time (seconds)")
    plt.legend()
    plt.grid()
    plt.show()

# Запуск экспериментов
run_experiments()
